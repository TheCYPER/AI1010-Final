{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31aa389f",
   "metadata": {},
   "source": [
    "\n",
    "# Feature Audit Notebook (after `preprocess.py`)\n",
    "\n",
    "This notebook:\n",
    "1) Loads your data and config  \n",
    "2) Fits **preprocessor** (from `preprocess.py`) on train-only  \n",
    "3) Produces **named, pandas** feature matrix for EDA  \n",
    "4) Runs essential **overfit diagnostics**: importance, drift, leakage checks  \n",
    "5) Gives visual summaries to guide next-step FE decisions\n",
    "\n",
    "> **Tip**: If your `scikit-learn` < 1.2 and `set_output(transform=\"pandas\")` fails, the notebook will fall back and still work (w/o column names). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50682364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainConfig(paths=Paths(train_csv='datas/office_train.csv', test_csv='datas/office_test.csv', pipeline_out='models/xgb_multiclass_pipeline.joblib', metrics_out='models/metrics.json', preds_out='models/predictions.csv'), cols=Columns(target='OfficeCategory', numeric=None, categorical=None), xgb=XGBParams(objective='multi:softprob', n_estimators=1000, learning_rate=0.03, max_depth=6, subsample=0.9, colsample_bytree=0.9, reg_lambda=5, reg_alpha=0, random_state=42, eval_metric='mlogloss'), test_size=0.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Imports & paths ---\n",
    "import os, sys, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Allow importing from your src/\n",
    "repo_root = Path.cwd()\n",
    "src_dir = (repo_root / \"src\")\n",
    "if not src_dir.exists():\n",
    "    # fallback: assume notebook is not in project root\n",
    "    # try one level up\n",
    "    src_dir = Path.cwd().parent / \"src\"\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from config import TrainConfig\n",
    "from preprocess import infer_column_types, build_preprocessor\n",
    "\n",
    "cfg = TrainConfig()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6befd672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 79) (7000, 79)\n",
      "OfficeCategory\n",
      "0    0.190714\n",
      "1    0.208964\n",
      "2    0.197321\n",
      "3    0.200357\n",
      "4    0.202643\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(cfg.paths.train_csv)\n",
    "target = cfg.cols.target\n",
    "if cfg.cols.numeric is None or cfg.cols.categorical is None:\n",
    "    num_cols, cat_cols = infer_column_types(df, target)\n",
    "else:\n",
    "    num_cols, cat_cols = cfg.cols.numeric, cfg.cols.categorical\n",
    "\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=cfg.test_size, random_state=42, stratify=y)\n",
    "print(X_tr.shape, X_va.shape)\n",
    "print(y_tr.value_counts(normalize=True).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d311a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] set_output to pandas failed -> fallback to numpy. Reason: Unable to configure output for WideFeatureBuilder() because `set_output` is not available.\n",
      "Transformed shapes: (28000, 297) (7000, 297)\n",
      "First 5 feature names: ['f0', 'f1', 'f2', 'f3', 'f4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/sklearn/preprocessing/_function_transformer.py:314: UserWarning: When `set_output` is configured to be 'pandas', `func` should return a pandas DataFrame to follow the `set_output` API  or `feature_names_out` should be defined.\n",
      "  warnings.warn(warn_msg.format(\"pandas\"))\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:203: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.where((den > 0) & (~np.isnan(den)), num / den, np.nan)\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:203: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.where((den > 0) & (~np.isnan(den)), num / den, np.nan)\n",
      "/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/sklearn/impute/_base.py:653: UserWarning: Skipping features without any observed values: [14 15 16]. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:98: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  total = dfc.groupby(c)[\"__y__\"].count()\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:99: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  by_cls = {k: dfc[dfc[\"__y__\"] == k].groupby(c)[\"__y__\"].count()\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:98: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  total = dfc.groupby(c)[\"__y__\"].count()\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:99: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  by_cls = {k: dfc[dfc[\"__y__\"] == k].groupby(c)[\"__y__\"].count()\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:98: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  total = dfc.groupby(c)[\"__y__\"].count()\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:99: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  by_cls = {k: dfc[dfc[\"__y__\"] == k].groupby(c)[\"__y__\"].count()\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:98: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  total = dfc.groupby(c)[\"__y__\"].count()\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:99: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  by_cls = {k: dfc[dfc[\"__y__\"] == k].groupby(c)[\"__y__\"].count()\n",
      "/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/sklearn/preprocessing/_function_transformer.py:314: UserWarning: When `set_output` is configured to be 'pandas', `func` should return a pandas DataFrame to follow the `set_output` API  or `feature_names_out` should be defined.\n",
      "  warnings.warn(warn_msg.format(\"pandas\"))\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:203: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.where((den > 0) & (~np.isnan(den)), num / den, np.nan)\n",
      "/Users/xueyi.wang/codes/ai1010-project/src/preprocess.py:203: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.where((den > 0) & (~np.isnan(den)), num / den, np.nan)\n",
      "/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/sklearn/impute/_base.py:653: UserWarning: Skipping features without any observed values: [14 15 16]. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/mnt/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFirst 5 feature names:\u001b[39m\u001b[33m\"\u001b[39m, feature_names[:\u001b[32m5\u001b[39m])\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Save head for quick look\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/mnt/data/preview_transformed_head.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved head -> /mnt/data/preview_transformed_head.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/pandas/core/generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/pandas/io/formats/csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ai1010-project/lib/python3.11/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '/mnt/data'"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Fit preprocessor on train only ---\n",
    "pre = build_preprocessor(num_cols, cat_cols)\n",
    "\n",
    "# Try to get pandas output (sklearn >=1.2); otherwise fallback to numpy\n",
    "use_pandas = True\n",
    "try:\n",
    "    pre.set_output(transform=\"pandas\")\n",
    "    X_tr_t = pre.fit_transform(X_tr, y_tr)\n",
    "    X_va_t = pre.transform(X_va)\n",
    "    feature_names = list(X_tr_t.columns)\n",
    "except Exception as e:\n",
    "    print(\"[warn] set_output to pandas failed -> fallback to numpy. Reason:\", e)\n",
    "    use_pandas = False\n",
    "    X_tr_t = pre.fit_transform(X_tr, y_tr)\n",
    "    X_va_t = pre.transform(X_va)\n",
    "    feature_names = [f\"f{i}\" for i in range(X_tr_t.shape[1])]\n",
    "\n",
    "print(\"Transformed shapes:\", X_tr_t.shape, X_va_t.shape)\n",
    "print(\"First 5 feature names:\", feature_names[:5])\n",
    "# Save head for quick look\n",
    "pd.DataFrame(X_tr_t, columns=feature_names).head(20).to_csv(\"mnt/data/preview_transformed_head.csv\", index=False)\n",
    "print(\"Saved head -> mnt/data/preview_transformed_head.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4f363",
   "metadata": {},
   "source": [
    "## 1) Original Feature Health Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbfbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Missingness in original features\n",
    "miss = X_tr.isna().mean().sort_values(ascending=False)\n",
    "display(miss.head(30))\n",
    "\n",
    "# Cardinality for categoricals\n",
    "card = X_tr[cat_cols].nunique(dropna=False).sort_values(ascending=False) if len(cat_cols) else pd.Series(dtype=int)\n",
    "display(card.head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdf641",
   "metadata": {},
   "source": [
    "## 2) Variance (Transformed Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr_df = pd.DataFrame(X_tr_t, columns=feature_names)\n",
    "var = Xtr_df.var().sort_values(ascending=False)\n",
    "display(var.head(30))\n",
    "low_var = (var <= 1e-6).sum()\n",
    "print(\"Low/near-constant features:\", low_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b34fc6",
   "metadata": {},
   "source": [
    "## 3) Correlation & Redundancy (numeric transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Only on numeric; here Xtr_df is numeric already\n",
    "corr = Xtr_df.corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(corr.values, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation heatmap (transformed)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated (>|0.95|)\n",
    "thr = 0.95\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), 1).astype(bool))\n",
    "high_corr = [c for c in upper.columns if any(upper[c].abs() > thr)]\n",
    "print(\"Highly correlated columns (>|0.95|), count:\", len(high_corr))\n",
    "print(high_corr[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69669f1",
   "metadata": {},
   "source": [
    "## 4) Fast Importance (XGB, strong regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "num_class = len(pd.unique(y_tr))\n",
    "xgb_light = XGBClassifier(\n",
    "    objective=\"multi:softprob\", num_class=num_class,\n",
    "    n_estimators=400, learning_rate=0.05,\n",
    "    max_depth=4, min_child_weight=6,\n",
    "    subsample=0.7, colsample_bytree=0.7,\n",
    "    reg_lambda=12.0, reg_alpha=1.0,\n",
    "    eval_metric=\"mlogloss\", random_state=42\n",
    ")\n",
    "xgb_light.fit(X_tr_t, y_tr, eval_set=[(X_va_t, y_va)], verbose=False)\n",
    "imp = getattr(xgb_light, \"feature_importances_\", np.zeros(X_tr_t.shape[1]))\n",
    "imp_s = pd.Series(imp, index=feature_names).sort_values(ascending=False)\n",
    "display(imp_s.head(40))\n",
    "print(\"Train acc:\", accuracy_score(y_tr, xgb_light.predict(X_tr_t)).round(4),\n",
    "      \" Valid acc:\", accuracy_score(y_va, xgb_light.predict(X_va_t)).round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dba5d3",
   "metadata": {},
   "source": [
    "## 5) Permutation Importance (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6facf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pi = permutation_importance(xgb_light, X_va_t, y_va, n_repeats=5, scoring=\"accuracy\", random_state=42)\n",
    "pi_mean = pd.Series(pi.importances_mean, index=feature_names).sort_values(ascending=False)\n",
    "display(pi_mean.head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05463b",
   "metadata": {},
   "source": [
    "## 6) Adversarial Validation (Train vs Valid Drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37532c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Z = np.vstack([np.asarray(X_tr_t), np.asarray(X_va_t)])\n",
    "d = np.r_[np.zeros(len(X_tr_t)), np.ones(len(X_va_t))]\n",
    "\n",
    "Z_tr, Z_te, d_tr, d_te = train_test_split(Z, d, test_size=0.3, stratify=d, random_state=42)\n",
    "adv = LogisticRegression(max_iter=2000)\n",
    "adv.fit(Z_tr, d_tr)\n",
    "p = adv.predict_proba(Z_te)[:,1]\n",
    "auc = roc_auc_score(d_te, p)\n",
    "print(f\"[Adversarial AUC] {auc:.4f}  (≈0.5 good; ≥0.7 drift)\")\n",
    "\n",
    "# Train-side validation probability (to build weights later if needed)\n",
    "p_train = adv.predict_proba(np.asarray(X_tr_t))[:,1]\n",
    "pd.Series(p_train).to_csv(\"/mnt/data/adv_prob_train.csv\", index=False)\n",
    "print(\"Saved train-side adv probs -> /mnt/data/adv_prob_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c97640",
   "metadata": {},
   "source": [
    "## 7) Leakage Probes (quick checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric features correlation with y (quick sanity; for multiclass use ANOVA-like score is better)\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = mutual_info_classif(np.asarray(X_tr_t), y_tr, discrete_features=False, random_state=42)\n",
    "mi_s = pd.Series(mi, index=feature_names).sort_values(ascending=False)\n",
    "display(mi_s.head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5c003",
   "metadata": {},
   "source": [
    "## 8) Save Ranked Lists for Manual Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f48838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imp_s.to_csv(\"/mnt/data/rank_xgb_importance.csv\")\n",
    "pi_mean.to_csv(\"/mnt/data/rank_permutation_importance.csv\")\n",
    "mi_s.to_csv(\"/mnt/data/rank_mutual_info.csv\")\n",
    "var.to_csv(\"/mnt/data/rank_variance.csv\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - /mnt/data/rank_xgb_importance.csv\")\n",
    "print(\" - /mnt/data/rank_permutation_importance.csv\")\n",
    "print(\" - /mnt/data/rank_mutual_info.csv\")\n",
    "print(\" - /mnt/data/rank_variance.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e5ebb",
   "metadata": {},
   "source": [
    "\n",
    "## 9) What to do next (Checklist)\n",
    "\n",
    "- **Drop**: low-variance (~0), ultra-high correlation (>0.95) duplicates, obvious leakage fields.  \n",
    "- **Encoding cleanup**: high-cardinality cols → pick **one** among Target Encoding (with strong smoothing) or Frequency Encoding. Low-cardinality → One-Hot. Rare categories → group to `__RARE__`.  \n",
    "- **Rebuild preprocessor**: with the reduced set; rerun notebook; verify overfit gap.  \n",
    "- **Group-wise add-back**: age/area/ratio/basement/quality/encodings in small groups; keep only groups with Δacc ≥ 0.2pp on validation.  \n",
    "- **(If drift high)**: use adversarial reweighting (1 - p_train) as `sample_weight` in training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
