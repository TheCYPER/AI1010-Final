# Architecture Documentation

## ðŸ—ï¸ System Architecture

This document describes the architecture of the ML pipeline, design decisions, and how components interact.

## Overview

The pipeline follows a **modular, layered architecture** inspired by production ML systems:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      main.py                             â”‚
â”‚                  (Orchestration Layer)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   configs/   â”‚   â”‚   training/  â”‚   â”‚   modeling/  â”‚
â”‚              â”‚   â”‚              â”‚   â”‚              â”‚
â”‚ â€¢ config.py  â”‚   â”‚ â€¢ trainer    â”‚   â”‚ â€¢ xgboost    â”‚
â”‚              â”‚   â”‚ â€¢ cv         â”‚   â”‚ â€¢ ensemble   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   feature_   â”‚   â”‚    data_     â”‚   â”‚hyperparameterâ”‚
â”‚ engineering/ â”‚   â”‚  cleaning/   â”‚   â”‚   _tuning/   â”‚
â”‚              â”‚   â”‚              â”‚   â”‚              â”‚
â”‚ â€¢ encoders   â”‚   â”‚ â€¢ missing    â”‚   â”‚ â€¢ optuna     â”‚
â”‚ â€¢ wide_feat  â”‚   â”‚ â€¢ outliers   â”‚   â”‚              â”‚
â”‚ â€¢ stat_feat  â”‚   â”‚              â”‚   â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    utils/    â”‚
                   â”‚              â”‚
                   â”‚ â€¢ logger     â”‚
                   â”‚ â€¢ metrics    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Design Principles

### 1. **Separation of Concerns**

Each module has a single, well-defined responsibility:

- **configs**: Configuration management only
- **data_cleaning**: Data quality operations
- **feature_engineering**: Feature creation and transformation
- **modeling**: Model definitions and wrappers
- **training**: Training orchestration
- **hyperparameter_tuning**: HPO logic
- **data_exploration**: EDA and auditing
- **utils**: Cross-cutting utilities

### 2. **Sklearn Compatibility**

All transformers follow the sklearn API:
- `fit(X, y)` - Learn from training data
- `transform(X)` - Apply transformation
- `fit_transform(X, y)` - Shortcut for both
- `get_feature_names_out()` - Return feature names

This enables:
- Easy integration with `ColumnTransformer`
- Pipeline composition
- Serialization with joblib
- Cross-validation compatibility

### 3. **Configuration-Driven**

All hyperparameters and settings live in `configs/config.py`:

```python
@dataclass
class Config:
    paths: PathConfig
    columns: ColumnConfig
    models: ModelConfig
    features: FeatureEngineeringConfig
    training: TrainingConfig
    tuning: HyperparameterTuningConfig
```

Benefits:
- Easy experimentation (change config, not code)
- Reproducibility (save config with model)
- No hardcoded values scattered across codebase

### 4. **Composability**

Components are designed to work independently or together:

```python
# Use preprocessor alone
preprocessor = build_preprocessor(num_cols, cat_cols)
X_transformed = preprocessor.fit_transform(X, y)

# Use model alone
model = XGBoostModel(config=params)
model.fit(X_transformed, y)

# Compose into pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', model.model_)
])
```

### 5. **Extensibility**

Adding new features is straightforward:

**New Feature Type:**
```python
# feature_engineering/my_features.py
class MyCustomFeatures(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        # Learn from data
        return self
    
    def transform(self, X):
        # Create features
        return new_features
```

**New Model:**
```python
# modeling/my_model.py
class MyModel(BaseModel):
    def build_model(self, **kwargs):
        # Initialize model
        pass
    
    def fit(self, X, y, **kwargs):
        # Train
        pass
    
    def predict(self, X):
        # Predict
        pass
```

## Component Details

### Feature Engineering Pipeline

The feature engineering pipeline is the most complex component:

```
Input DataFrame
       â”‚
       â”œâ”€â†’ Numeric Branch
       â”‚      â”œâ”€â†’ Median Imputation + Indicators
       â”‚      â””â”€â†’ Log1p Transform (PlotSize)
       â”‚
       â”œâ”€â†’ Categorical Branch (One-Hot)
       â”‚      â””â”€â†’ Missing â†’ "__MISSING__" â†’ OneHot
       â”‚
       â”œâ”€â†’ Frequency Encoding Branch
       â”‚      â””â”€â†’ Map categories â†’ frequency
       â”‚
       â”œâ”€â†’ Target Encoding Branch
       â”‚      â””â”€â†’ Map categories â†’ P(y|category) with smoothing
       â”‚
       â”œâ”€â†’ Wide Features Branch
       â”‚      â”œâ”€â†’ Age features (BuildingAge, etc.)
       â”‚      â”œâ”€â†’ Area features (TotalLivingArea, etc.)
       â”‚      â”œâ”€â†’ Ratio features (PlotCoverage, etc.)
       â”‚      â”œâ”€â†’ Quality features (OverallQuality, etc.)
       â”‚      â”œâ”€â†’ Temporal features (SeasonListed, etc.)
       â”‚      â”œâ”€â†’ Interaction features (QualityAreaProximity)
       â”‚      â””â”€â†’ Domain knowledge (RoomSizeAdequacy, etc.)
       â”‚
       â”œâ”€â†’ Statistical Aggregation Branch
       â”‚      â”œâ”€â†’ Group by ZoningClassification
       â”‚      â”œâ”€â†’ Group by BuildingType
       â”‚      â”œâ”€â†’ Compute z-scores within groups
       â”‚      â””â”€â†’ Compute relative shifts
       â”‚
       â””â”€â†’ Business Missing Indicator
              â””â”€â†’ ConferenceRoomQuality missing â†’ 0/1
       
       â†“
Column Transformer concatenates all branches
       â†“
Final Feature Matrix (300-400 features)
```

### Training Flow

**Single Split Training:**

```
1. Load data
2. Train/val split (stratified)
3. Build preprocessor
4. Fit preprocessor on train only
5. Transform train and val
6. Compute sample weights (if enabled)
7. Train model with eval_set
8. Evaluate on both sets
9. Save pipeline + metrics
```

**Cross-Validation:**

```
1. Load data
2. Create StratifiedKFold splitter
3. For each fold:
   a. Split data
   b. Build preprocessor
   c. Fit on fold train
   d. Transform fold train and val
   e. Train model
   f. Evaluate
   g. Store results
4. Aggregate results (mean Â± std)
5. Save summary
```

### Hyperparameter Tuning Flow

```
1. Load data
2. Build preprocessor (fit once)
3. Create Optuna study
4. For each trial:
   a. Sample hyperparameters
   b. Build model with sampled params
   c. Cross-validate on train set
   d. Return mean CV score
5. Select best parameters
6. Save results
```

## Data Flow

### Training Data Flow

```
CSV File
   â”‚
   â–¼
DataFrame (X, y)
   â”‚
   â”œâ”€â†’ Train (80%)
   â”‚      â”‚
   â”‚      â”œâ”€â†’ Preprocessor.fit(X_train, y)
   â”‚      â”‚      â”‚
   â”‚      â”‚      â””â”€â†’ Learn encodings, statistics, etc.
   â”‚      â”‚
   â”‚      â”œâ”€â†’ Preprocessor.transform(X_train)
   â”‚      â”‚      â”‚
   â”‚      â”‚      â””â”€â†’ X_train_transformed (300-400 features)
   â”‚      â”‚
   â”‚      â””â”€â†’ Model.fit(X_train_transformed, y_train)
   â”‚
   â””â”€â†’ Val (20%)
          â”‚
          â””â”€â†’ Preprocessor.transform(X_val)
                 â”‚
                 â””â”€â†’ X_val_transformed
                        â”‚
                        â””â”€â†’ Model.predict(X_val_transformed)
                               â”‚
                               â””â”€â†’ Evaluation Metrics
```

### Test Prediction Flow

```
Test CSV
   â”‚
   â–¼
Test DataFrame
   â”‚
   â””â”€â†’ Pipeline.predict(X_test)
          â”‚
          â”œâ”€â†’ Preprocessor.transform(X_test)
          â”‚      â”‚
          â”‚      â””â”€â†’ X_test_transformed
          â”‚
          â””â”€â†’ Model.predict(X_test_transformed)
                 â”‚
                 â””â”€â†’ Predictions
                        â”‚
                        â””â”€â†’ submission.csv
```

## Key Design Decisions

### 1. Why ColumnTransformer?

**Decision:** Use sklearn's `ColumnTransformer` for preprocessing

**Rationale:**
- Handles different column types elegantly
- Preserves column names (with `set_output(transform="pandas")`)
- Integrates seamlessly with `Pipeline`
- Serializable with joblib

**Trade-off:** Slightly more verbose than custom code, but much more maintainable

### 2. Why Separate Encoding Strategies?

**Decision:** Multiple encoding branches (frequency, target, one-hot)

**Rationale:**
- Different cardinality â†’ different optimal encoding
- Target encoding for high-cardinality predictive features
- Frequency encoding for medium-cardinality
- One-hot for low-cardinality
- Prevents over-parameterization

### 3. Why Wide Features?

**Decision:** Create 40+ derived features upfront

**Rationale:**
- Tree models benefit from explicit interactions
- Domain knowledge >> automatic feature learning
- Interpretability (know what model uses)
- Faster than neural auto-feature learning

**Alternative:** Could use auto-feature engineering (autofeat), but:
- Less interpretable
- Can create too many features
- Computationally expensive

### 4. Why Multiple Imputation Strategies?

**Decision:** Median for numeric, constant for categorical

**Rationale:**
- Median robust to outliers
- Constant ("__MISSING__") preserves signal in missingness
- Add indicators to capture missing pattern importance

### 5. Why Sample Weighting?

**Decision:** Optional class-weighted training

**Rationale:**
- Office categories may be imbalanced
- Weighting helps model focus on rare classes
- Configurable (can enable/disable)

### 6. Why Optuna for Tuning?

**Decision:** Use Optuna over GridSearch/RandomSearch

**Rationale:**
- Bayesian optimization more efficient
- Supports pruning (early stopping of bad trials)
- Beautiful visualizations
- Can resume interrupted searches

**Trade-off:** Additional dependency, but worth it for speed

## Performance Considerations

### Memory Efficiency

1. **Avoid duplicate data**
   - Use `copy()` only when necessary
   - Transform in-place where possible

2. **Sparse matrices**
   - OneHotEncoder can use sparse (disabled for simplicity)
   - Could enable for very high cardinality

3. **Batch processing**
   - For very large datasets, could add batch processing
   - Current: assumes data fits in memory

### Computational Efficiency

1. **Parallel processing**
   - XGBoost: `n_jobs=-1` (uses all cores)
   - Cross-validation: could parallelize folds (not implemented)
   - Hyperparameter tuning: `n_jobs=-1` in CV

2. **Early stopping**
   - Prevents unnecessary training
   - Monitors validation loss

3. **Caching**
   - Preprocessor fit once, transform multiple times
   - Could add more aggressive caching

## Testing Strategy

### Unit Tests (Recommended)

```python
# tests/test_encoders.py
def test_frequency_encoder():
    X = pd.DataFrame({'col': ['a', 'b', 'a', 'c']})
    enc = FrequencyEncoder(cols=['col'])
    enc.fit(X)
    result = enc.transform(X)
    assert result.shape == (4, 1)
    assert result[0, 0] == 0.5  # 'a' appears 2/4 times
```

### Integration Tests

```python
# tests/test_pipeline.py
def test_full_pipeline():
    # Load data
    # Build pipeline
    # Train
    # Predict
    # Assert accuracy > threshold
```

### Property-Based Tests

```python
# tests/test_transformers.py
def test_transformer_shape_preservation():
    # Given any valid input
    # When transformed
    # Then n_rows unchanged
```

## Future Improvements

### Short Term

1. **Add more encoders**
   - Weight of Evidence (WOE)
   - Count encoding
   - Hashing

2. **Feature selection**
   - Automated feature selection based on importance
   - Remove correlated features

3. **More models**
   - LightGBM
   - CatBoost
   - Neural networks

### Medium Term

1. **Experiment tracking**
   - MLflow integration
   - Weights & Biases

2. **Model monitoring**
   - Drift detection in production
   - Performance degradation alerts

3. **Automated retraining**
   - Scheduled retraining
   - Trigger-based retraining

### Long Term

1. **Cloud deployment**
   - Docker containers
   - Kubernetes orchestration
   - API serving

2. **Real-time inference**
   - Streaming predictions
   - Low-latency serving

3. **AutoML integration**
   - Automated architecture search
   - Automated feature engineering

## Comparison to Original WXYVer Code

### Improvements

| Aspect | WXYVer | New Architecture |
|--------|--------|------------------|
| **Organization** | Single monolithic files | Modular structure |
| **Reusability** | Hard to reuse components | Easy to mix & match |
| **Testability** | Difficult to test | Each component testable |
| **Configuration** | Scattered parameters | Centralized config |
| **Documentation** | Inline comments | Comprehensive docs |
| **Extensibility** | Hard to extend | Plugin-like architecture |

### Preserved Features

- âœ… All feature engineering logic
- âœ… Target encoding with smoothing
- âœ… Statistical aggregations
- âœ… Wide feature builder
- âœ… Class weighting
- âœ… Cross-validation support

### Migration Path

Old code still works in `WXYVer/`. New code coexists:

```
AI1010Final/
â”œâ”€â”€ WXYVer/           # Original code (unchanged)
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ models/
â”‚
â”œâ”€â”€ configs/          # New modular architecture
â”œâ”€â”€ feature_engineering/
â”œâ”€â”€ modeling/
â”œâ”€â”€ training/
â””â”€â”€ main.py
```

Users can:
1. Keep using WXYVer for experiments
2. Migrate gradually to new architecture
3. Compare results between both

---

**Questions or suggestions?** Feel free to extend this architecture!

