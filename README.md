# Office Category Prediction - ML Pipeline

A modular, production-ready machine learning pipeline for office category classification.

## ğŸ“ Project Structure

```
AI1010Final/
â”œâ”€â”€ configs/                    # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py              # Centralized configuration
â”‚
â”œâ”€â”€ data_cleaning/             # Data cleaning utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ column_types.py        # Column type inference
â”‚   â”œâ”€â”€ missing_handler.py     # Missing value handling
â”‚   â””â”€â”€ outlier_handler.py     # Outlier detection & handling
â”‚
â”œâ”€â”€ data_exploration/          # EDA and feature auditing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ exploratory_analysis.py # Basic EDA
â”‚   â””â”€â”€ feature_audit.py       # Feature importance & drift analysis
â”‚
â”œâ”€â”€ feature_engineering/       # Feature engineering modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ encoders.py           # Frequency & target encoding
â”‚   â”œâ”€â”€ wide_features.py      # Derived feature builder
â”‚   â”œâ”€â”€ statistical_features.py # Statistical aggregations
â”‚   â”œâ”€â”€ transformers.py       # Log transforms, etc.
â”‚   â””â”€â”€ preprocessor.py       # Main preprocessor assembly
â”‚
â”œâ”€â”€ modeling/                  # Model definitions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_model.py         # Abstract base class
â”‚   â”œâ”€â”€ xgboost_model.py      # XGBoost wrapper
â”‚   â””â”€â”€ ensemble.py           # Ensemble methods
â”‚
â”œâ”€â”€ training/                  # Training logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py            # Single split trainer
â”‚   â””â”€â”€ cross_validator.py   # K-fold cross-validation
â”‚
â”œâ”€â”€ hyperparameter_tuning/    # Hyperparameter optimization
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ tuner.py             # Optuna-based tuning
â”‚
â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py            # Logging utilities
â”‚   â””â”€â”€ metrics.py           # Evaluation metrics
â”‚
â”œâ”€â”€ datasets/                 # Data directory
â”‚   â”œâ”€â”€ office_train.csv
â”‚   â””â”€â”€ office_test.csv
â”‚
â”œâ”€â”€ outputs/                  # Output directory (created automatically)
â”‚   â”œâ”€â”€ models/              # Trained models
â”‚   â”œâ”€â”€ metrics/             # Evaluation metrics
â”‚   â”œâ”€â”€ predictions/         # Test predictions
â”‚   â””â”€â”€ logs/               # Log files
â”‚
â”œâ”€â”€ main.py                  # Main entry point
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Exploratory Data Analysis

```bash
python main.py --mode eda
```

This will:
- Analyze missing values
- Show target distribution
- Compute cardinality
- Generate statistics
- Save report to `outputs/eda_report.json`

### 3. Train Model (Single Split)

```bash
python main.py --mode train
```

This will:
- Load and split data (80/20)
- Build preprocessing pipeline
- Train XGBoost model
- Evaluate on validation set
- Save model to `outputs/models/pipeline.joblib`
- Save metrics to `outputs/metrics/metrics.json`

### 4. Cross-Validation Training

```bash
python main.py --mode cv
```

This will:
- Perform 5-fold stratified cross-validation
- Train model on each fold
- Aggregate results with mean Â± std
- Save summary to `outputs/models/cv/cv_summary.json`

### 5. Hyperparameter Tuning

```bash
python main.py --mode tune
```

This will:
- Use Optuna for Bayesian optimization
- Run 100 trials (configurable)
- Save best parameters to `outputs/tuning_results.json`

### 6. Feature Audit

```bash
python main.py --mode audit
```

This will:
- Compute feature importance
- Run permutation importance
- Check for train/val drift (adversarial validation)
- Identify highly correlated features
- Save report to `outputs/feature_audit.json`

### 7. Make Predictions

```bash
python main.py --mode predict --model_path outputs/models/pipeline.joblib
```

This will:
- Load trained model
- Make predictions on test set
- Save to `outputs/predictions/submission.csv`

## ğŸ”§ Configuration

All configuration is centralized in `configs/config.py`. Key sections:

### Paths
```python
train_csv = "datasets/office_train.csv"
test_csv = "datasets/office_test.csv"
output_dir = "outputs"
```

### Model Parameters
```python
xgb_params = {
    'n_estimators': 1500,
    'learning_rate': 0.06,
    'max_depth': 4,
    'subsample': 0.75,
    'colsample_bytree': 0.55,
    'reg_lambda': 10.0,
    'reg_alpha': 3.0,
    ...
}
```

### Feature Engineering
```python
freq_encoding_cols = ['RoofType', 'ExteriorCovering1', 'FoundationType']
target_encoding_cols = ['ZoningClassification', 'BuildingType', ...]
```

### Training
```python
test_size = 0.2
n_splits = 5
use_class_weights = True
use_early_stopping = True
```

## ğŸ§ª Feature Engineering Pipeline

The pipeline includes:

1. **Missing Value Handling**
   - Median imputation for numeric features
   - Constant imputation for categorical features
   - Missing indicators

2. **Encoding**
   - Frequency encoding for high-cardinality features
   - Target encoding with Laplace smoothing
   - One-hot encoding for low-cardinality features

3. **Wide Features** (40+ derived features)
   - Age features: BuildingAge, YearsSinceRenovation
   - Area features: TotalLivingArea, TotalBasementArea
   - Ratio features: PlotCoverage, RoomDensity, etc.
   - Quality combinations: OverallQuality, ExteriorScore
   - Temporal features: SeasonListed, BuildingLifeStage
   - Interaction features: QualityAreaProximity
   - Domain knowledge: RoomSizeAdequacy, ParkingAdequacy

4. **Statistical Aggregations**
   - Group-level z-scores
   - Relative shifts from group mean

5. **Transformations**
   - Log1p for skewed features (PlotSize)

## ğŸ“Š Model Performance

The pipeline is optimized for accuracy with:
- Stratified sampling
- Class weighting for imbalanced data
- Early stopping to prevent overfitting
- Comprehensive regularization (L1/L2)

Expected validation accuracy: **~75-80%** (depending on feature selection and tuning)

## ğŸ”¬ Advanced Usage

### Custom Configuration

Create a custom config and pass it:

```python
from configs import Config

config = Config()
config.models.xgb_params['n_estimators'] = 2000
config.training.n_splits = 10

# Use in your code
from training import Trainer
trainer = Trainer(config)
trainer.run()
```

### Programmatic API

You can also use the modules programmatically:

```python
from configs import Config
from training import Trainer
from modeling import XGBoostModel

# Setup
config = Config()
trainer = Trainer(config)

# Load data
X, y = trainer.load_data()
X_train, X_val, y_train, y_val = trainer.split_data(X, y)

# Build preprocessor
trainer.build_preprocessor(X_train)

# Train
model = XGBoostModel(config=config.models.xgb_params)
results = trainer.train(model, X_train, y_train, X_val, y_val)
```

### Adding New Features

Extend `WideFeatureBuilder` in `feature_engineering/wide_features.py`:

```python
def _add_custom_features(self, df: pd.DataFrame, out: Dict[str, Any]):
    """Add your custom features."""
    # Example: Interaction between two features
    out["CustomFeature"] = df["Feature1"] * df["Feature2"]
```

## ğŸ“ Development Notes

### Design Principles

1. **Modularity**: Each component is self-contained and reusable
2. **Extensibility**: Easy to add new features, models, or strategies
3. **Configurability**: Centralized configuration for easy experimentation
4. **Sklearn Compatibility**: All transformers follow sklearn API
5. **Production Ready**: Proper logging, error handling, serialization

### Testing

```bash
# Run tests (if you add them)
pytest tests/

# Test individual modules
python -c "from configs import Config; print(Config())"
```

### Adding New Models

1. Create model class in `modeling/` inheriting from `BaseModel`
2. Implement `build_model()`, `fit()`, `predict()` methods
3. Update `main.py` to support new model

Example:

```python
from modeling import BaseModel

class MyCustomModel(BaseModel):
    def build_model(self, **kwargs):
        # Your model initialization
        pass
    
    def fit(self, X, y, **kwargs):
        # Training logic
        pass
    
    def predict(self, X):
        # Prediction logic
        pass
```

## ğŸ› Troubleshooting

### Common Issues

1. **Import errors**: Ensure virtual environment is activated
2. **Memory issues**: Reduce `n_estimators` or use sampling
3. **Optuna not found**: Install with `pip install optuna`

### Performance Tips

- Use `n_jobs=-1` for parallel processing
- Enable early stopping to save time
- Start with fewer CV folds (e.g., 3) during development
- Use hyperparameter tuning sparingly (time-consuming)

## ğŸ“š References

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Scikit-learn](https://scikit-learn.org/)
- [Optuna](https://optuna.org/)

## ğŸ¤ Contributing

Feel free to:
- Add new feature engineering techniques
- Implement additional models
- Improve hyperparameter search spaces
- Add visualization utilities
- Write tests

## ğŸ“„ License

This project is for educational purposes.

---

**Happy Modeling! ğŸ‰**

